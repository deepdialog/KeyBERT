[![PyPI - Python](https://img.shields.io/badge/python-3.6%20|%203.7%20|%203.8-blue.svg)](https://pypi.org/project/keybert/)
[![PyPI - License](https://img.shields.io/badge/license-MIT-green.svg)](https://github.com/MaartenGr/keybert/blob/master/LICENSE)
[![Build](https://img.shields.io/github/workflow/status/MaartenGr/keyBERT/Code%20Checks/master)](https://pypi.org/project/keybert/)

# ZhKeyBERT

Based on [KeyBERT](https://github.com/MaartenGr/KeyBERT), enhance the keyword
extraction model for the characteristics of Chinese.

Corresponding medium post can be found [here](https://towardsdatascience.com/keyword-extraction-with-bert-724efca412ea).

<a name="toc"/></a>
## Table of Contents  
<!--ts-->
   1. [About the Project](#about)  
   2. [Getting Started](#gettingstarted)    
        2.1. [Installation](#installation)    
        2.2. [Basic Usage](#usage)     
        2.3. [Max Sum Similarity](#maxsum)  
        2.4. [Maximal Marginal Relevance](#maximal)  
        2.5. [Embedding Models](#embeddings)
<!--te-->


<a name="about"/></a>
## 1. About the Project
[Back to ToC](#toc)  

Although there are already many methods available for keyword generation 
(e.g., 
[Rake](https://github.com/aneesha/RAKE), 
[YAKE!](https://github.com/LIAAD/yake), TF-IDF, etc.) 
I wanted to create a very basic, but powerful method for extracting keywords and keyphrases. 
This is where **KeyBERT** comes in! Which uses BERT-embeddings and simple cosine similarity
to find the sub-phrases in a document that are the most similar to the document itself.

First, document embeddings are extracted with BERT to get a document-level representation. 
Then, word embeddings are extracted for N-gram words/phrases. Finally, we use cosine similarity 
to find the words/phrases that are the most similar to the document. The most similar words could 
then be identified as the words that best describe the entire document.  

KeyBERT is by no means unique and is created as a quick and easy method
for creating keywords and keyphrases. Although there are many great 
papers and solutions out there that use BERT-embeddings 
(e.g., 
[1](https://github.com/pranav-ust/BERT-keyphrase-extraction),
[2](https://github.com/ibatra/BERT-Keyword-Extractor),
[3](https://www.preprints.org/manuscript/201908.0073/download/final_file),
), I could not find a BERT-based solution that did not have to be trained from scratch and
could be used for beginners (**correct me if I'm wrong!**).
Thus, the goal was a `pip install keybert` and at most 3 lines of code in usage.   

<a name="gettingstarted"/></a>
## 2. Getting Started
[Back to ToC](#toc)  

<a name="installation"/></a>
###  2.1. Installation

<a name="usage"/></a>
###  2.2. Usage

The most minimal example can be seen below for the extraction of keywords:
```python
from keybert import KeyBERT, extract_kws

docs = """æ—¶å€¼10æœˆ25æ—¥æŠ—ç¾æ´æœçºªå¿µæ—¥ï¼Œã€Šé•¿æ´¥æ¹–ã€‹ç‰‡æ–¹å‘å¸ƒäº†â€œçºªå¿µä¸­å›½äººæ°‘å¿—æ„¿å†›æŠ—ç¾æ´æœå‡ºå›½ä½œæˆ˜71å‘¨å¹´ç‰¹åˆ«çŸ­ç‰‡â€ï¼Œå†æ¬¡å‘ä¼Ÿå¤§çš„å¿—æ„¿å†›è‡´æ•¬ï¼
ç”µå½±ã€Šé•¿æ´¥æ¹–ã€‹å…¨æƒ…å…¨æ™¯åœ°è¿˜åŸäº†71å¹´å‰æŠ—ç¾æ´æœæˆ˜åœºä¸Šé‚£åœºå²è¯—æˆ˜å½¹ï¼Œå¿—æ„¿å†›å¥‹ä¸é¡¾èº«çš„è‹±å‹‡ç²¾ç¥ä»¤è§‚ä¼—æ„Ÿå¹ï¼šâ€œå²æœˆå³¥åµ˜è‹±é›„ä¸ç­ï¼Œä¸¹å¿ƒé“éª¨å†›é­‚æ°¸å­˜ï¼â€å½±ç‰‡ä¸Šæ˜ ä»¥æ¥ç¥¨æˆ¿å±¡åˆ›æ–°é«˜ï¼Œç›®å‰çªç ´53äº¿å…ƒï¼Œæš‚åˆ—ä¸­å›½å½±å²ç¥¨æˆ¿æ€»æ¦œç¬¬ä¸‰åã€‚
å€¼å¾—ä¸€æçš„æ˜¯ï¼Œè¿™éƒ¨å½±ç‰‡çš„å¾ˆå¤šä¸»åˆ›æˆ–æœ‰å†›äººçš„è¡€è„‰ï¼Œæˆ–æœ‰å½“å…µçš„ç»å†ï¼Œæˆ–è€…å®¶äººæ˜¯å†›äººã€‚æèµ·è¿™äº›ä»–ä»¬ä¹Ÿå……æ»¡è‡ªè±ªï¼Œå½±ç‰‡æ€»ç›‘åˆ¶é»„å»ºæ–°ç§°ï¼šâ€œå½“å…µä»¥åä¼šæœ‰ä¸€ç§ç‰¹åˆ«èƒ½åšæŒçš„åŠ²å„¿ã€‚â€é¥°æ¼”é›·å…¬çš„èƒ¡å†›é€éœ²ï¼šâ€œæˆ‘çˆ¶äº²æ›¾ç»å‚åŠ è¿‡æŠ—ç¾æ´æœï¼Œè¿˜å¾—äº†ä¸€ä¸ªä¸‰ç­‰åŠŸã€‚â€å½±ç‰‡å†å²é¡¾é—®ç‹æ ‘å¢è¡¨ç¤ºï¼šâ€œæˆ‘å½“äº†äº”åå¤šå¹´çš„å…µï¼Œæˆ‘çš„è€éƒ¨é˜Ÿå°±æ˜¯ä¸Šç”˜å²­ä¸Šä¸‹æ¥çš„ï¼Œé‚£äº›è€å…µéƒ½æ˜¯æˆ‘çš„å¶åƒã€‚â€
â€œèº«å…ˆå£«å’å«åå¤å®¶å›½ï¼Œè¡€æˆ˜æ— ç•æŠ¤å±±æ²³æ— æ™ã€‚â€ç‰‡ä¸­é¥°æ¼”ä¸ƒè¿è¿é•¿ä¼åƒé‡Œçš„å´äº¬æ„Ÿå¹ï¼šâ€œè¦æ°¸è¿œè®°ä½è¿™äº›å…ˆçƒˆä»¬ï¼Œä»–ä»¬ç»™æˆ‘ä»¬å¸¦æ¥ä»Šå¤©çš„å’Œå¹³ã€‚æ„Ÿè°¢ä»–ä»¬çš„ä»˜å‡ºï¼Œæ‰è®©æˆ‘ä»¬æœ‰ä»Šå¤©çš„å¹¸ç¦ç”Ÿæ´»ã€‚â€é¥°æ¼”æ–°å…µä¼ä¸‡é‡Œçš„æ˜“çƒŠåƒçºè¡¨ç¤ºï¼šâ€œæˆ˜äº‰çš„æ®‹é…·ã€ç¢¾å‹å¼çš„ä¼¤å®³ï¼Œå…¶å®æˆ‘ä»¬ç°åœ¨çš„å¹´è½»äººå‡ ä¹å¾ˆéš¾èƒ½ä½“ä¼šåˆ°ï¼Œå¸Œæœ›å¤§å®¶çœ‹å®Œç”µå½±åèƒ½æ˜ç™½ï¼Œæ˜¯é‚£äº›å…ˆè¾ˆä»¬çš„ç‰ºç‰²å¥‰çŒ®ï¼Œæ¢æ¥äº†æˆ‘ä»¬çš„ç°åœ¨ã€‚â€
å½±ç‰‡å¯¹æˆ˜äº‰ç¾¤åƒçš„æ¢å¼˜å‘ˆç°ï¼Œå¯¹ä¸ªä½“å‘½è¿çš„æ·±åˆ‡å…³æ€€ï¼Œä»¤è®¸å¤šè§‚ä¼—æ— æ³•æ§åˆ¶è‡ªå·±çš„çœ¼æ³ªï¼Œè§‚ä¼—ç§°ï¼šâ€œå½“çœ‹åˆ°å½±ç‰‡ä¸­çš„æƒŠé™©æˆ˜æ–—åœºé¢ï¼Œçœ‹åˆ°è‹±é›„ä»¬å£®æ€€æ¿€çƒˆçš„æ‹¼æ€ï¼Œä¸ºå›½æèº¯çš„è‹±å‹‡æ— ç•å’Œæ— æ‚”ä»˜å‡ºï¼Œæˆ‘æ˜ç™½äº†ä¸ºä»€ä¹ˆè¯´ä»Šå¤©çš„å¹¸ç¦ç”Ÿæ´»æ¥ä¹‹ä¸æ˜“ã€‚â€ï¼ˆè®°è€… ç‹é‡‘è·ƒï¼‰"""
kw_model = KeyBERT(model='paraphrase-multilingual-MiniLM-L12-v2')
extract_kws_zh(docs, kw_model)
```

Comparison
```python
>>> extract_kws_zh(docs, kw_model)

[('çºªå¿µä¸­å›½äººæ°‘å¿—æ„¿å†›æŠ—ç¾æ´æœ', 0.7034),
 ('ç”µå½±é•¿æ´¥æ¹–', 0.6285),
 ('å‘¨å¹´ç‰¹åˆ«çŸ­ç‰‡', 0.5668),
 ('çºªå¿µä¸­å›½äººæ°‘å¿—æ„¿å†›', 0.6894),
 ('ä½œæˆ˜71å‘¨å¹´', 0.5637)]
>>> import jieba; kw_model.extract_keywords(' '.join(jieba.cut(docs)), keyphrase_ngram_range=(1, 3), 
                                            use_mmr=True, diversity=0.25)

[('æŠ—ç¾æ´æœ çºªå¿µæ—¥ é•¿æ´¥æ¹–', 0.796),
 ('çºªå¿µ ä¸­å›½äººæ°‘å¿—æ„¿å†› æŠ—ç¾æ´æœ', 0.7577),
 ('ä½œæˆ˜ 71 å‘¨å¹´', 0.6126),
 ('25 æŠ—ç¾æ´æœ çºªå¿µæ—¥', 0.635),
 ('è‡´æ•¬ ç”µå½± é•¿æ´¥æ¹–', 0.6514)]
```

You can set `ngram_range`, whose default value is `(1, 3)`,
to set the length of the resulting keywords/keyphrases:

```python
>>> extract_kws_zh(docs, kw_model, ngram_range=(1, 1))
[('ä¸­å›½äººæ°‘å¿—æ„¿å†›', 0.6094),
 ('é•¿æ´¥æ¹–', 0.505),
 ('å‘¨å¹´', 0.4504),
 ('å½±ç‰‡', 0.447),
 ('è‹±é›„', 0.4297)]
```

```python
>>> extract_kws_zh(docs, kw_model, ngram_range=(1, 2))
[('çºªå¿µä¸­å›½äººæ°‘å¿—æ„¿å†›', 0.6894),
 ('ç”µå½±é•¿æ´¥æ¹–', 0.6285),
 ('å¹´å‰æŠ—ç¾æ´æœ', 0.476),
 ('ä¸­å›½äººæ°‘å¿—æ„¿å†›æŠ—ç¾æ´æœ', 0.6349),
 ('ä¸­å›½å½±å²', 0.5534)]
``` 

**NOTE**: For a full overview of all possible transformer models see [sentence-transformer](https://www.sbert.net/docs/pretrained_models.html).
I would advise `"paraphrase-multilingual-MiniLM-L12-v2"` Chinese documents for efficiency
and acceptable accuracy.

<a name="maximal"/></a>
###  2.3. Maximal Marginal Relevance

It's recommended to use Maximal Margin Relevance (MMR) for diversity by
setting the optional parameter `use_mmr`, which is `True` in default.  
To diversify the results, we can use MMR to create
keywords / keyphrases which is also based on cosine similarity. The results 
with **high diversity**:

```python
>>> extract_kws_zh(docs, kw_model, use_mmr = True, diversity=0.7)

[('çºªå¿µä¸­å›½äººæ°‘å¿—æ„¿å†›æŠ—ç¾æ´æœ', 0.7034),
 ('è§‚ä¼—æ— æ³•æ§åˆ¶è‡ªå·±', 0.1212),
 ('å±±æ²³æ— æ™', 0.2233),
 ('å½±ç‰‡ä¸Šæ˜ ä»¥æ¥', 0.5427),
 ('53äº¿å…ƒ', 0.3287)]
``` 

The results with **low diversity**:  

```python
>>> kw_model.extract_keywords(doc, keyphrase_ngram_range=(3, 3), stop_words='english', 
                              use_mmr=True, diversity=0.2)
[('çºªå¿µä¸­å›½äººæ°‘å¿—æ„¿å†›æŠ—ç¾æ´æœ', 0.7034),
 ('ç”µå½±é•¿æ´¥æ¹–', 0.6285),
 ('çºªå¿µä¸­å›½äººæ°‘å¿—æ„¿å†›', 0.6894),
 ('å‘¨å¹´ç‰¹åˆ«çŸ­ç‰‡', 0.5668),
 ('ä½œæˆ˜71å‘¨å¹´', 0.5637)]
``` 

And the default and recommended `diversity` is `0.25`.

<a name="embeddings"/></a>
###  2.4. Embedding Models
KeyBERT supports many embedding models that can be used to embed the documents and words:

* Sentence-Transformers
* Flair
* Spacy
* Gensim
* USE

Click [here](https://maartengr.github.io/KeyBERT/guides/embeddings.html) for a full overview of all supported embedding models.

**Sentence-Transformers**  
You can select any model from `sentence-transformers` [here](https://www.sbert.net/docs/pretrained_models.html) 
and pass it through KeyBERT with `model`:

```python
from keybert import KeyBERT
kw_model = KeyBERT(model='all-MiniLM-L6-v2')
```

Or select a SentenceTransformer model with your own parameters:

```python
from keybert import KeyBERT
from sentence_transformers import SentenceTransformer

sentence_model = SentenceTransformer("all-MiniLM-L6-v2")
kw_model = KeyBERT(model=sentence_model)
```

**Flair**  
[Flair](https://github.com/flairNLP/flair) allows you to choose almost any embedding model that 
is publicly available. Flair can be used as follows:

```python
from keybert import KeyBERT
from flair.embeddings import TransformerDocumentEmbeddings

roberta = TransformerDocumentEmbeddings('roberta-base')
kw_model = KeyBERT(model=roberta)
```

You can select any ğŸ¤— transformers model [here](https://huggingface.co/models).


## Citation
To cite KeyBERT in your work, please use the following bibtex reference:

```bibtex
@misc{grootendorst2020keybert,
  author       = {Maarten Grootendorst},
  title        = {KeyBERT: Minimal keyword extraction with BERT.},
  year         = 2020,
  publisher    = {Zenodo},
  version      = {v0.3.0},
  doi          = {10.5281/zenodo.4461265},
  url          = {https://doi.org/10.5281/zenodo.4461265}
}
```

## References
Below, you can find several resources that were used for the creation of KeyBERT 
but most importantly, these are amazing resources for creating impressive keyword extraction models: 

**Papers**:  
* Sharma, P., & Li, Y. (2019). [Self-Supervised Contextual Keyword and Keyphrase Retrieval with Self-Labelling.](https://www.preprints.org/manuscript/201908.0073/download/final_file)

**Github Repos**:  
* https://github.com/thunlp/BERT-KPE
* https://github.com/ibatra/BERT-Keyword-Extractor
* https://github.com/pranav-ust/BERT-keyphrase-extraction
* https://github.com/swisscom/ai-research-keyphrase-extraction

**MMR**:  
The selection of keywords/keyphrases was modeled after:
* https://github.com/swisscom/ai-research-keyphrase-extraction

**NOTE**: If you find a paper or github repo that has an easy-to-use implementation
of BERT-embeddings for keyword/keyphrase extraction, let me know! I'll make sure to
add a reference to this repo. 

